{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For P5 we've been instructed to create a Convolutional Neural Network to classify the Cifar-10 dataset using Keras. The notebook is divided in several parts:\n",
    "\n",
    "<ol>\n",
    "<li>1: Necessary preparations</li>\n",
    "<li>2: Data Preparation</li>\n",
    "<li>3: A first model</li>\n",
    "<li>???</li>\n",
    "<li>5: References</li>\n",
    "</ol>\n",
    "\n",
    "I'll work through each one individually, and elaborate more:\n",
    "\n",
    "<h3>1: Necessary preparations</h3>\n",
    "\n",
    "We need to take some simple steps to get ready to create our model. First we need to import some libraries and classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, ZeroPadding2D, Activation, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras uses the numpy random module to instantiate a model's weights and biases. To make sure that notebook behaviour is replicable, we need to set the random seed to a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load in the dataset. The dataset is pre-split in a training and testing set, so we assign both the target and feature variables for the test and training set to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Even though x_train and x_test are matrices and not vectors, and convention might expect the variable names to be capital letters, this is how the Keras examples do it, and how I'll do it.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, my NVidia-GPU is currently lined up for surgery and out of commission. Even though I do have an AMD-GPU, using that for Tensorflow/Keras requires a Linux installation, and skills with said Linux installation. I have neither of those, so I'll be training using a CPU. Have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2: Data Preparation</h2>\n",
    "\n",
    "Before any actual model can be trained, the dataset needs some work. Let's start with the images.\n",
    "\n",
    "<h4>Images</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 32, 32, 3)"
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are represented as Numpy Arrays. According to the Keras documentation, there are 3 32x32 matrices for every picture, representing the RGB color-scales. Every one of these matrices contain pixel values from 0 up to 255. The train set contains 50.000 images, and the test set 10.000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[[ 59,  62,  63],\n         [ 43,  46,  45],\n         [ 50,  48,  43],\n         ...,\n         [158, 132, 108],\n         [152, 125, 102],\n         [148, 124, 103]],\n\n        [[ 16,  20,  20],\n         [  0,   0,   0],\n         [ 18,   8,   0],\n         ...,\n         [123,  88,  55],\n         [119,  83,  50],\n         [122,  87,  57]],\n\n        [[ 25,  24,  21],\n         [ 16,   7,   0],\n         [ 49,  27,   8],\n         ...,\n         [118,  84,  50],\n         [120,  84,  50],\n         [109,  73,  42]],\n\n        ...,\n\n        [[208, 170,  96],\n         [201, 153,  34],\n         [198, 161,  26],\n         ...,\n         [160, 133,  70],\n         [ 56,  31,   7],\n         [ 53,  34,  20]],\n\n        [[180, 139,  96],\n         [173, 123,  42],\n         [186, 144,  30],\n         ...,\n         [184, 148,  94],\n         [ 97,  62,  34],\n         [ 83,  53,  34]],\n\n        [[177, 144, 116],\n         [168, 129,  94],\n         [179, 142,  87],\n         ...,\n         [216, 184, 140],\n         [151, 118,  84],\n         [123,  92,  72]]],\n\n\n       [[[154, 177, 187],\n         [126, 137, 136],\n         [105, 104,  95],\n         ...,\n         [ 91,  95,  71],\n         [ 87,  90,  71],\n         [ 79,  81,  70]],\n\n        [[140, 160, 169],\n         [145, 153, 154],\n         [125, 125, 118],\n         ...,\n         [ 96,  99,  78],\n         [ 77,  80,  62],\n         [ 71,  73,  61]],\n\n        [[140, 155, 164],\n         [139, 146, 149],\n         [115, 115, 112],\n         ...,\n         [ 79,  82,  64],\n         [ 68,  70,  55],\n         [ 67,  69,  55]],\n\n        ...,\n\n        [[175, 167, 166],\n         [156, 154, 160],\n         [154, 160, 170],\n         ...,\n         [ 42,  34,  36],\n         [ 61,  53,  57],\n         [ 93,  83,  91]],\n\n        [[165, 154, 128],\n         [156, 152, 130],\n         [159, 161, 142],\n         ...,\n         [103,  93,  96],\n         [123, 114, 120],\n         [131, 121, 131]],\n\n        [[163, 148, 120],\n         [158, 148, 122],\n         [163, 156, 133],\n         ...,\n         [143, 133, 139],\n         [143, 134, 142],\n         [143, 133, 144]]],\n\n\n       [[[255, 255, 255],\n         [253, 253, 253],\n         [253, 253, 253],\n         ...,\n         [253, 253, 253],\n         [253, 253, 253],\n         [253, 253, 253]],\n\n        [[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]],\n\n        [[255, 255, 255],\n         [254, 254, 254],\n         [254, 254, 254],\n         ...,\n         [254, 254, 254],\n         [254, 254, 254],\n         [254, 254, 254]],\n\n        ...,\n\n        [[113, 120, 112],\n         [111, 118, 111],\n         [105, 112, 106],\n         ...,\n         [ 72,  81,  80],\n         [ 72,  80,  79],\n         [ 72,  80,  79]],\n\n        [[111, 118, 110],\n         [104, 111, 104],\n         [ 99, 106,  98],\n         ...,\n         [ 68,  75,  73],\n         [ 70,  76,  75],\n         [ 78,  84,  82]],\n\n        [[106, 113, 105],\n         [ 99, 106,  98],\n         [ 95, 102,  94],\n         ...,\n         [ 78,  85,  83],\n         [ 79,  85,  83],\n         [ 80,  86,  84]]],\n\n\n       ...,\n\n\n       [[[ 35, 178, 235],\n         [ 40, 176, 239],\n         [ 42, 176, 241],\n         ...,\n         [ 99, 177, 219],\n         [ 79, 147, 197],\n         [ 89, 148, 189]],\n\n        [[ 57, 182, 234],\n         [ 44, 184, 250],\n         [ 50, 183, 240],\n         ...,\n         [156, 182, 200],\n         [141, 177, 206],\n         [116, 149, 175]],\n\n        [[ 98, 197, 237],\n         [ 64, 189, 252],\n         [ 69, 192, 245],\n         ...,\n         [188, 195, 206],\n         [119, 135, 147],\n         [ 61,  79,  90]],\n\n        ...,\n\n        [[ 73,  79,  77],\n         [ 53,  63,  68],\n         [ 54,  68,  80],\n         ...,\n         [ 17,  40,  64],\n         [ 21,  36,  51],\n         [ 33,  48,  49]],\n\n        [[ 61,  68,  75],\n         [ 55,  70,  86],\n         [ 57,  79, 103],\n         ...,\n         [ 24,  48,  72],\n         [ 17,  35,  53],\n         [  7,  23,  32]],\n\n        [[ 44,  56,  73],\n         [ 46,  66,  88],\n         [ 49,  77, 105],\n         ...,\n         [ 27,  52,  77],\n         [ 21,  43,  66],\n         [ 12,  31,  50]]],\n\n\n       [[[189, 211, 240],\n         [186, 208, 236],\n         [185, 207, 235],\n         ...,\n         [175, 195, 224],\n         [172, 194, 222],\n         [169, 194, 220]],\n\n        [[194, 210, 239],\n         [191, 207, 236],\n         [190, 206, 235],\n         ...,\n         [173, 192, 220],\n         [171, 191, 218],\n         [167, 190, 216]],\n\n        [[208, 219, 244],\n         [205, 216, 240],\n         [204, 215, 239],\n         ...,\n         [175, 191, 217],\n         [172, 190, 216],\n         [169, 191, 215]],\n\n        ...,\n\n        [[207, 199, 181],\n         [203, 195, 175],\n         [203, 196, 173],\n         ...,\n         [135, 132, 127],\n         [162, 158, 150],\n         [168, 163, 151]],\n\n        [[198, 190, 170],\n         [189, 181, 159],\n         [180, 172, 147],\n         ...,\n         [178, 171, 160],\n         [175, 169, 156],\n         [175, 169, 154]],\n\n        [[198, 189, 173],\n         [189, 181, 162],\n         [178, 170, 149],\n         ...,\n         [195, 184, 169],\n         [196, 189, 171],\n         [195, 190, 171]]],\n\n\n       [[[229, 229, 239],\n         [236, 237, 247],\n         [234, 236, 247],\n         ...,\n         [217, 219, 233],\n         [221, 223, 234],\n         [222, 223, 233]],\n\n        [[222, 221, 229],\n         [239, 239, 249],\n         [233, 234, 246],\n         ...,\n         [223, 223, 236],\n         [227, 228, 238],\n         [210, 211, 220]],\n\n        [[213, 206, 211],\n         [234, 232, 239],\n         [231, 233, 244],\n         ...,\n         [220, 220, 232],\n         [220, 219, 232],\n         [202, 203, 215]],\n\n        ...,\n\n        [[150, 143, 135],\n         [140, 135, 127],\n         [132, 127, 120],\n         ...,\n         [224, 222, 218],\n         [230, 228, 225],\n         [241, 241, 238]],\n\n        [[137, 132, 126],\n         [130, 127, 120],\n         [125, 121, 115],\n         ...,\n         [181, 180, 178],\n         [202, 201, 198],\n         [212, 211, 207]],\n\n        [[122, 119, 114],\n         [118, 116, 110],\n         [120, 116, 111],\n         ...,\n         [179, 177, 173],\n         [164, 164, 162],\n         [163, 163, 161]]]], dtype=uint8)"
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks work best with inputs from 0 to 1. The easiest way to prepare the dataset for usage in a Neural Network is by dividing all pixel values by 255, so that they properly scaled, in the desired range. We accomplish this with numpy vectorized operations, which apply the specified operation element-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 1)"
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[[0.23137255, 0.24313725, 0.24705882],\n         [0.16862745, 0.18039216, 0.17647059],\n         [0.19607843, 0.18823529, 0.16862745],\n         ...,\n         [0.61960784, 0.51764706, 0.42352941],\n         [0.59607843, 0.49019608, 0.4       ],\n         [0.58039216, 0.48627451, 0.40392157]],\n\n        [[0.0627451 , 0.07843137, 0.07843137],\n         [0.        , 0.        , 0.        ],\n         [0.07058824, 0.03137255, 0.        ],\n         ...,\n         [0.48235294, 0.34509804, 0.21568627],\n         [0.46666667, 0.3254902 , 0.19607843],\n         [0.47843137, 0.34117647, 0.22352941]],\n\n        [[0.09803922, 0.09411765, 0.08235294],\n         [0.0627451 , 0.02745098, 0.        ],\n         [0.19215686, 0.10588235, 0.03137255],\n         ...,\n         [0.4627451 , 0.32941176, 0.19607843],\n         [0.47058824, 0.32941176, 0.19607843],\n         [0.42745098, 0.28627451, 0.16470588]],\n\n        ...,\n\n        [[0.81568627, 0.66666667, 0.37647059],\n         [0.78823529, 0.6       , 0.13333333],\n         [0.77647059, 0.63137255, 0.10196078],\n         ...,\n         [0.62745098, 0.52156863, 0.2745098 ],\n         [0.21960784, 0.12156863, 0.02745098],\n         [0.20784314, 0.13333333, 0.07843137]],\n\n        [[0.70588235, 0.54509804, 0.37647059],\n         [0.67843137, 0.48235294, 0.16470588],\n         [0.72941176, 0.56470588, 0.11764706],\n         ...,\n         [0.72156863, 0.58039216, 0.36862745],\n         [0.38039216, 0.24313725, 0.13333333],\n         [0.3254902 , 0.20784314, 0.13333333]],\n\n        [[0.69411765, 0.56470588, 0.45490196],\n         [0.65882353, 0.50588235, 0.36862745],\n         [0.70196078, 0.55686275, 0.34117647],\n         ...,\n         [0.84705882, 0.72156863, 0.54901961],\n         [0.59215686, 0.4627451 , 0.32941176],\n         [0.48235294, 0.36078431, 0.28235294]]],\n\n\n       [[[0.60392157, 0.69411765, 0.73333333],\n         [0.49411765, 0.5372549 , 0.53333333],\n         [0.41176471, 0.40784314, 0.37254902],\n         ...,\n         [0.35686275, 0.37254902, 0.27843137],\n         [0.34117647, 0.35294118, 0.27843137],\n         [0.30980392, 0.31764706, 0.2745098 ]],\n\n        [[0.54901961, 0.62745098, 0.6627451 ],\n         [0.56862745, 0.6       , 0.60392157],\n         [0.49019608, 0.49019608, 0.4627451 ],\n         ...,\n         [0.37647059, 0.38823529, 0.30588235],\n         [0.30196078, 0.31372549, 0.24313725],\n         [0.27843137, 0.28627451, 0.23921569]],\n\n        [[0.54901961, 0.60784314, 0.64313725],\n         [0.54509804, 0.57254902, 0.58431373],\n         [0.45098039, 0.45098039, 0.43921569],\n         ...,\n         [0.30980392, 0.32156863, 0.25098039],\n         [0.26666667, 0.2745098 , 0.21568627],\n         [0.2627451 , 0.27058824, 0.21568627]],\n\n        ...,\n\n        [[0.68627451, 0.65490196, 0.65098039],\n         [0.61176471, 0.60392157, 0.62745098],\n         [0.60392157, 0.62745098, 0.66666667],\n         ...,\n         [0.16470588, 0.13333333, 0.14117647],\n         [0.23921569, 0.20784314, 0.22352941],\n         [0.36470588, 0.3254902 , 0.35686275]],\n\n        [[0.64705882, 0.60392157, 0.50196078],\n         [0.61176471, 0.59607843, 0.50980392],\n         [0.62352941, 0.63137255, 0.55686275],\n         ...,\n         [0.40392157, 0.36470588, 0.37647059],\n         [0.48235294, 0.44705882, 0.47058824],\n         [0.51372549, 0.4745098 , 0.51372549]],\n\n        [[0.63921569, 0.58039216, 0.47058824],\n         [0.61960784, 0.58039216, 0.47843137],\n         [0.63921569, 0.61176471, 0.52156863],\n         ...,\n         [0.56078431, 0.52156863, 0.54509804],\n         [0.56078431, 0.5254902 , 0.55686275],\n         [0.56078431, 0.52156863, 0.56470588]]],\n\n\n       [[[1.        , 1.        , 1.        ],\n         [0.99215686, 0.99215686, 0.99215686],\n         [0.99215686, 0.99215686, 0.99215686],\n         ...,\n         [0.99215686, 0.99215686, 0.99215686],\n         [0.99215686, 0.99215686, 0.99215686],\n         [0.99215686, 0.99215686, 0.99215686]],\n\n        [[1.        , 1.        , 1.        ],\n         [1.        , 1.        , 1.        ],\n         [1.        , 1.        , 1.        ],\n         ...,\n         [1.        , 1.        , 1.        ],\n         [1.        , 1.        , 1.        ],\n         [1.        , 1.        , 1.        ]],\n\n        [[1.        , 1.        , 1.        ],\n         [0.99607843, 0.99607843, 0.99607843],\n         [0.99607843, 0.99607843, 0.99607843],\n         ...,\n         [0.99607843, 0.99607843, 0.99607843],\n         [0.99607843, 0.99607843, 0.99607843],\n         [0.99607843, 0.99607843, 0.99607843]],\n\n        ...,\n\n        [[0.44313725, 0.47058824, 0.43921569],\n         [0.43529412, 0.4627451 , 0.43529412],\n         [0.41176471, 0.43921569, 0.41568627],\n         ...,\n         [0.28235294, 0.31764706, 0.31372549],\n         [0.28235294, 0.31372549, 0.30980392],\n         [0.28235294, 0.31372549, 0.30980392]],\n\n        [[0.43529412, 0.4627451 , 0.43137255],\n         [0.40784314, 0.43529412, 0.40784314],\n         [0.38823529, 0.41568627, 0.38431373],\n         ...,\n         [0.26666667, 0.29411765, 0.28627451],\n         [0.2745098 , 0.29803922, 0.29411765],\n         [0.30588235, 0.32941176, 0.32156863]],\n\n        [[0.41568627, 0.44313725, 0.41176471],\n         [0.38823529, 0.41568627, 0.38431373],\n         [0.37254902, 0.4       , 0.36862745],\n         ...,\n         [0.30588235, 0.33333333, 0.3254902 ],\n         [0.30980392, 0.33333333, 0.3254902 ],\n         [0.31372549, 0.3372549 , 0.32941176]]],\n\n\n       ...,\n\n\n       [[[0.1372549 , 0.69803922, 0.92156863],\n         [0.15686275, 0.69019608, 0.9372549 ],\n         [0.16470588, 0.69019608, 0.94509804],\n         ...,\n         [0.38823529, 0.69411765, 0.85882353],\n         [0.30980392, 0.57647059, 0.77254902],\n         [0.34901961, 0.58039216, 0.74117647]],\n\n        [[0.22352941, 0.71372549, 0.91764706],\n         [0.17254902, 0.72156863, 0.98039216],\n         [0.19607843, 0.71764706, 0.94117647],\n         ...,\n         [0.61176471, 0.71372549, 0.78431373],\n         [0.55294118, 0.69411765, 0.80784314],\n         [0.45490196, 0.58431373, 0.68627451]],\n\n        [[0.38431373, 0.77254902, 0.92941176],\n         [0.25098039, 0.74117647, 0.98823529],\n         [0.27058824, 0.75294118, 0.96078431],\n         ...,\n         [0.7372549 , 0.76470588, 0.80784314],\n         [0.46666667, 0.52941176, 0.57647059],\n         [0.23921569, 0.30980392, 0.35294118]],\n\n        ...,\n\n        [[0.28627451, 0.30980392, 0.30196078],\n         [0.20784314, 0.24705882, 0.26666667],\n         [0.21176471, 0.26666667, 0.31372549],\n         ...,\n         [0.06666667, 0.15686275, 0.25098039],\n         [0.08235294, 0.14117647, 0.2       ],\n         [0.12941176, 0.18823529, 0.19215686]],\n\n        [[0.23921569, 0.26666667, 0.29411765],\n         [0.21568627, 0.2745098 , 0.3372549 ],\n         [0.22352941, 0.30980392, 0.40392157],\n         ...,\n         [0.09411765, 0.18823529, 0.28235294],\n         [0.06666667, 0.1372549 , 0.20784314],\n         [0.02745098, 0.09019608, 0.1254902 ]],\n\n        [[0.17254902, 0.21960784, 0.28627451],\n         [0.18039216, 0.25882353, 0.34509804],\n         [0.19215686, 0.30196078, 0.41176471],\n         ...,\n         [0.10588235, 0.20392157, 0.30196078],\n         [0.08235294, 0.16862745, 0.25882353],\n         [0.04705882, 0.12156863, 0.19607843]]],\n\n\n       [[[0.74117647, 0.82745098, 0.94117647],\n         [0.72941176, 0.81568627, 0.9254902 ],\n         [0.7254902 , 0.81176471, 0.92156863],\n         ...,\n         [0.68627451, 0.76470588, 0.87843137],\n         [0.6745098 , 0.76078431, 0.87058824],\n         [0.6627451 , 0.76078431, 0.8627451 ]],\n\n        [[0.76078431, 0.82352941, 0.9372549 ],\n         [0.74901961, 0.81176471, 0.9254902 ],\n         [0.74509804, 0.80784314, 0.92156863],\n         ...,\n         [0.67843137, 0.75294118, 0.8627451 ],\n         [0.67058824, 0.74901961, 0.85490196],\n         [0.65490196, 0.74509804, 0.84705882]],\n\n        [[0.81568627, 0.85882353, 0.95686275],\n         [0.80392157, 0.84705882, 0.94117647],\n         [0.8       , 0.84313725, 0.9372549 ],\n         ...,\n         [0.68627451, 0.74901961, 0.85098039],\n         [0.6745098 , 0.74509804, 0.84705882],\n         [0.6627451 , 0.74901961, 0.84313725]],\n\n        ...,\n\n        [[0.81176471, 0.78039216, 0.70980392],\n         [0.79607843, 0.76470588, 0.68627451],\n         [0.79607843, 0.76862745, 0.67843137],\n         ...,\n         [0.52941176, 0.51764706, 0.49803922],\n         [0.63529412, 0.61960784, 0.58823529],\n         [0.65882353, 0.63921569, 0.59215686]],\n\n        [[0.77647059, 0.74509804, 0.66666667],\n         [0.74117647, 0.70980392, 0.62352941],\n         [0.70588235, 0.6745098 , 0.57647059],\n         ...,\n         [0.69803922, 0.67058824, 0.62745098],\n         [0.68627451, 0.6627451 , 0.61176471],\n         [0.68627451, 0.6627451 , 0.60392157]],\n\n        [[0.77647059, 0.74117647, 0.67843137],\n         [0.74117647, 0.70980392, 0.63529412],\n         [0.69803922, 0.66666667, 0.58431373],\n         ...,\n         [0.76470588, 0.72156863, 0.6627451 ],\n         [0.76862745, 0.74117647, 0.67058824],\n         [0.76470588, 0.74509804, 0.67058824]]],\n\n\n       [[[0.89803922, 0.89803922, 0.9372549 ],\n         [0.9254902 , 0.92941176, 0.96862745],\n         [0.91764706, 0.9254902 , 0.96862745],\n         ...,\n         [0.85098039, 0.85882353, 0.91372549],\n         [0.86666667, 0.8745098 , 0.91764706],\n         [0.87058824, 0.8745098 , 0.91372549]],\n\n        [[0.87058824, 0.86666667, 0.89803922],\n         [0.9372549 , 0.9372549 , 0.97647059],\n         [0.91372549, 0.91764706, 0.96470588],\n         ...,\n         [0.8745098 , 0.8745098 , 0.9254902 ],\n         [0.89019608, 0.89411765, 0.93333333],\n         [0.82352941, 0.82745098, 0.8627451 ]],\n\n        [[0.83529412, 0.80784314, 0.82745098],\n         [0.91764706, 0.90980392, 0.9372549 ],\n         [0.90588235, 0.91372549, 0.95686275],\n         ...,\n         [0.8627451 , 0.8627451 , 0.90980392],\n         [0.8627451 , 0.85882353, 0.90980392],\n         [0.79215686, 0.79607843, 0.84313725]],\n\n        ...,\n\n        [[0.58823529, 0.56078431, 0.52941176],\n         [0.54901961, 0.52941176, 0.49803922],\n         [0.51764706, 0.49803922, 0.47058824],\n         ...,\n         [0.87843137, 0.87058824, 0.85490196],\n         [0.90196078, 0.89411765, 0.88235294],\n         [0.94509804, 0.94509804, 0.93333333]],\n\n        [[0.5372549 , 0.51764706, 0.49411765],\n         [0.50980392, 0.49803922, 0.47058824],\n         [0.49019608, 0.4745098 , 0.45098039],\n         ...,\n         [0.70980392, 0.70588235, 0.69803922],\n         [0.79215686, 0.78823529, 0.77647059],\n         [0.83137255, 0.82745098, 0.81176471]],\n\n        [[0.47843137, 0.46666667, 0.44705882],\n         [0.4627451 , 0.45490196, 0.43137255],\n         [0.47058824, 0.45490196, 0.43529412],\n         ...,\n         [0.70196078, 0.69411765, 0.67843137],\n         [0.64313725, 0.64313725, 0.63529412],\n         [0.63921569, 0.63921569, 0.63137255]]]])"
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Targets</h4>\n",
    "\n",
    "The targets need work too. Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[6],\n       [9],\n       [9],\n       ...,\n       [9],\n       [1],\n       [1]], dtype=uint8)"
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The targets consist of numbers 0-9, each representing one type of target. Because a loss function might interpret this as ordinal, discrete of continuous data, even though these are completely separate and non-ordered categories, we can one-hot encode them and make sure our later network has 10 outputs to more appropriately represent the meaning of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_classes = np.unique(y_train).size\n",
    "\n",
    "y_train, y_test = to_categorical(y_train, n_classes), to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 1.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works.\n",
    "\n",
    "<h2>3: A first model</h2>\n",
    "\n",
    "Like mentioned before, we'll be constructing a Convolutional neural network. This is more appropriate for the task at hand, then the \"traditional\" Neural network that we made before, because it more easily learns the concept of \"proximity\" in the images.\n",
    "\n",
    "The most important resource for determining the architecture used is Aldewereld et al., 2016. From the suggested template for usual Layer patterns, I have arrived at the following:\n",
    "\n",
    "For 2 repetitions, there are 2 sequential convolutional layers, followed by a max-pooling layer. This is followed by 3 fully connected layers. Every part will be elaborated on in more detail when they're added.\n",
    "\n",
    "The amount of kernels in every layer is in multiplying factors of 2, to maintain optimal performance through vectorized operations. According to Karpathy, a (2016), it's quite usual for layers to become deeper and deeper.\n",
    "\n",
    "In general, the Convolutional part of the network maintains the dimensions of it's inputs, as suggested by Karpathy, A (2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with 2 convolutional layers, with small (3x3) filter sizes, in an attempt to pick up on small patterns in the images. The padding=same ensures that input shape (width and height) is the same as output shape. There are followed by a ReLu activation function, because it's proved to be quite effective at what it's supposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32, 32, 3), padding=\"same\"))\n",
    "base_model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model.add(Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\"))\n",
    "base_model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale down from 32x32 to 16x16 using a max pooling layer. This is a fairly standard followup to one or more convolutional layers, according to both Aldewereld et al. and Karpathy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model.add(MaxPool2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this we add 2 more convolutional layers. This time, the kernel size is bigger, in an attempt to pick up on more general patterns. We zero-pad with 2 on each side this time (different padding because of different kernel size) to ensure that output height and width remains equal to input. Because of a <a href=\"https://stackoverflow.com/questions/45013060/how-to-do-zero-padding-in-keras-conv-layer\">Keras Quirk</a>, this has to be done more explicitly for P=2. These layers still use ReLu for obvious reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model.add(ZeroPadding2D(2))\n",
    "base_model.add(Conv2D(filters=128, kernel_size=(5, 5)))\n",
    "base_model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model.add(ZeroPadding2D(2))\n",
    "base_model.add(Conv2D(filters=256, kernel_size=(5, 5)))\n",
    "base_model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, we scale down from 16x16 to 8x8 using a max-pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model.add(MaxPool2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To move onto usage of a densely connected part of the network, we need to flatten everything. We can easily accomplish this by adding a flatten layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model.add(Flatten())  # 8 x 8 x 256 = 16.384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the last layer, our network has 16.384 parameters. We'll start scaling down that number through subsequent fully connected layers (still with ReLu activation functions) in powers of 2 for performance reasons.\n",
    "\n",
    "We'll end with a layer of 10 neurons as output layer, because we're classifying 10 categories. These will use the softmax activation function, so that the outputs more accurately represent how confident the network is about it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model.add(Dense(1024))\n",
    "base_model.add(Activation(\"relu\"))\n",
    "base_model.add(Dense(512))\n",
    "base_model.add(Activation(\"relu\"))\n",
    "base_model.add(Dense(n_classes))\n",
    "base_model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compile the model with 2 loss-metrics: categorical cross entropy and accuracy. Categorical crossentropy for the actual learning, so that the model has a loss function that incrementally lowers as the network approaches as local mininum on it, and can compute useful gradients. We'll the accuracy as a final metric, because it's more human-proof, and more accurately represents how well the model might be performing in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_37 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 16, 16, 128)       204928    \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1024)              16778240  \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 18,351,946\n",
      "Trainable params: 18,351,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that through the convolutional layers, we have indeed succeeded in maintaining input and output-shape, which is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we've been instructed not to add anything extra besides the required basics for the model, I will take some liberties and make the model stop training automatically in case the Accuracy on the validation set doesn't improve after an epoch, because training on my CPU takes forever and I would also like to be able to easily evaluate loss and accuracy on the test-set before the model gets over fitted, without any workarounds like saving the model to disk every epoch. I justify this as not actually changing anything in the model, just interrupting training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor=\"val_accuracy\", patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train for a maximum of 10 epochs (because more if just unmanageable time-wise without my GPU), with a batch-size of 128. We split 20% of the data from the training set, as validation set, to monitor for over fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 37/313 [==>...........................] - ETA: 3:02 - loss: 4.2358 - accuracy: 0.1237"
     ]
    }
   ],
   "source": [
    "base_epochs = 10  # Store amount of epochs in seperate variable for plotting purposes later\n",
    "base_hist = base_model.fit(x_train, y_train, batch_size = 128, epochs=base_epochs, verbose=1, validation_split = 0.2, callbacks=[callback])#, validation_steps=12000//128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we can already see the numbers, a better insight into the model's training journey might be provided by viewing a plot of the accuracy on both the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(base_hist.history[\"accuracy\"], \"rx-\", label=\"Training Dataset Accuracy\")\n",
    "plt.plot(base_hist.history[\"val_accuracy\"], \"bx-\", label=\"Validation Dataset Accuracy\")\n",
    "\n",
    "if len(base_hist.history[\"val_accuracy\"]) - 1 < base_epochs:  # If amount of actual epochs is not equal to expected amount of epochs.\n",
    "    plt.vlines([len(base_hist.history[\"val_accuracy\"]) - 1],\n",
    "               colors=\"y\", linestyles=\"dashed\",\n",
    "               ymin=0, ymax=1,\n",
    "               label=\"Training interruption\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(range(base_epochs))\n",
    "plt.title(\"Accuracy after every epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cab see that even though training dataset accuracy keeps rising, the accuracy on the validation set only gets better with small amounts after 2 epochs. It does continue to rise, when examining the exact numbers. After the 5th epoch, the validation dataset accuracy finally stops rising, and training is interrupted to prevent over fitting. We can examine the model's final performance by evaluating it against the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "_, accuracy = base_model.evaluate(x_test, y_test)\n",
    "f\"Final test-set accuracy is {accuracy}%.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with an accuracy of 75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3>References</h3>\n",
    "\n",
    "Keras Team. Keras Documentation: CIFAR10 small images classification dataset. Keras. Retrieved March 26, 2022, from https://keras.io/api/datasets/cifar10/\n",
    "\n",
    "Aldewereld, H., van der Bijl, B., Bunk, J., van Moergestel, L. 2022. Machine Learning (reader).\n",
    "Utrecht: Hogeschool Utrecht.\n",
    "\n",
    "Karpathy, A. [Andrej Karpathy]. (2016, 28 January). CS231n Winter 2016: Lecture 7: Convolutional Neural Networks Video. Youtube. https://www.youtube.com/watch?v=LxfUGhug-iQ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}