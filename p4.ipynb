{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For P4, we've implemented the backpropagation algorithm  for Sigmoid neurons, which is implemented in the Python source code that we'll import in a minute. All that remains is verifying the implementation to a series of use-cases, from easy to difficult. We'll perform the following tasks through backpropagation-trained fully connected neural networks:\n",
    "\n",
    "1. Create an AND-gate.\n",
    "2. Create a XOR-gate.\n",
    "3. Create a half-adder.\n",
    "4. Classify the Iris dataset.\n",
    "5. Classify the Digit dataset.\n",
    "\n",
    "I won't be using any fancy testing frameworks because I've been steered away from that explicitly.\n",
    "\n",
    "### 0: Setup.\n",
    "\n",
    "I'll import my own source code through the ml package, and import some (builtin) libraries. We'll also lock down the random seed, so that we can guarantee reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2022)\n",
    "\n",
    "import itertools\n",
    "from typing import List, Tuple\n",
    "\n",
    "import ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll also _borrow_ a function from a previous assignment for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def binary_input_space(length: int) -> List[Tuple[int]]:\n",
    "    \"\"\"Compute all possible binary input combinations with a certain length.\n",
    "\n",
    "    Args:\n",
    "        length: length of the combinations.\n",
    "\n",
    "    Returns:\n",
    "        All possible binary input combinations of a certain length.\"\"\"\n",
    "    return list(itertools.product([0, 1], repeat=length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Creating an AND-gate\n",
    "\n",
    "Similarly to earlier Perceptron assignments, one Neuron should be enough to create an AND-gate. To show the components a network consists of, I'll initialize this network explicitly without fancy weight initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_and_neuron = ml.OutputNeuron(\n",
    "    weights=[-1, 1],\n",
    "    bias=0,\n",
    "    learning_rate=1,\n",
    "    activation_function=ml.sigmoid\n",
    ")\n",
    "\n",
    "_and_layer = ml.NeuronLayer(\n",
    "    [_and_neuron]\n",
    ")\n",
    "\n",
    "and_network = ml.NeuronNetwork(\n",
    "    [_and_layer]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define some inputs and outputs that the AND-gate would be expected to take in, and return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (1, 0), (1, 1)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_2 = binary_input_space(2)\n",
    "binary_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "and_target = [\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the network in the simplest way you can imagine: for 1000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "and_network.train(\n",
    "    binary_2,\n",
    "    and_target,\n",
    "    iterations=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And review how it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (0, 0). Network made prediction [0.00023702819264548002] (rounded 0). Should be [0].\n",
      "Input: (0, 1). Network made prediction [0.05512421225431237] (rounded 0). Should be [0].\n",
      "Input: (1, 0). Network made prediction [0.05531142025660785] (rounded 0). Should be [0].\n",
      "Input: (1, 1). Network made prediction [0.9350968562675207] (rounded 1). Should be [1].\n"
     ]
    }
   ],
   "source": [
    "for inp, target in zip(binary_2, and_target):\n",
    "    prediction = and_network.feed_forward(inp)\n",
    "    print(f\"Input: {inp}. Network made prediction {prediction} (rounded {round(prediction[0])}). Should be {target}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily see that the network gave outputs corresponding to an AND-gate for all inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Creating a XOR-gate.\n",
    "\n",
    "The XOR-gate tests our implementation of the backpropagation algorithm more, because (just like with the Perceptron networks) this requires using a hidden layer. To showcase the more high-level controls of the ml package, we'll construct a network with random weights through a convenience method. We can also use Xavier initialization here to make sure we don't get stuck on the fading gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xor_network = ml.NeuronNetwork.build_network(\n",
    "    layer_layout=[2, 2, 1],\n",
    "    learning_rate=1,\n",
    "    activation_function=ml.sigmoid,\n",
    "    xavier_initialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same inputs as for the AND-gate, though we do need some new targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xor_target = [\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the example assignments use this method, and I used those to debug my algorithm and I know it works, we'll also train this network for 1000 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xor_network.train(\n",
    "    binary_2,\n",
    "    xor_target,\n",
    "    iterations=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (0, 0). Network made prediction [0.11130851601286407] (rounded 0). Should be [0].\n",
      "Input: (0, 1). Network made prediction [0.8274363743744205] (rounded 1). Should be [1].\n",
      "Input: (1, 0). Network made prediction [0.8172625164665333] (rounded 1). Should be [1].\n",
      "Input: (1, 1). Network made prediction [0.24237938520746624] (rounded 0). Should be [0].\n"
     ]
    }
   ],
   "source": [
    "for inp, target in zip(binary_2, xor_target):\n",
    "    prediction = xor_network.feed_forward(inp)\n",
    "    print(f\"Input: {inp}. Network made prediction {prediction} (rounded {round(prediction[0])}). Should be {target}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the network was able to learn how to perform it's task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Creating a half adder.\n",
    "\n",
    "By now, you know the procedure. Like in P1, we'll be using 2 layers: a hidden layer with 3 Neurons, and an output layer with 2 Neurons. We can use the convenience method again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ha_network = ml.NeuronNetwork.build_network(\n",
    "    layer_layout=[2, 3, 2],\n",
    "    learning_rate=1,\n",
    "    activation_function=ml.sigmoid,\n",
    "    xavier_initialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same 2-length binary input space, we use the following target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ha_target = [\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [1, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ha_network.train(\n",
    "    binary_2,\n",
    "    ha_target,\n",
    "    iterations=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: (0, 0). Network made prediction [0.00030338392106541123, 0.042582553286565475] (rounded [0, 0]). Should be [0, 0].\n",
      "Input: (0, 1). Network made prediction [0.027922475243593384, 0.9555367891841755] (rounded [0, 1]). Should be [0, 1].\n",
      "Input: (1, 0). Network made prediction [0.0269761997382146, 0.9546387092874814] (rounded [0, 1]). Should be [0, 1].\n",
      "Input: (1, 1). Network made prediction [0.9535109460899509, 0.055095633390166494] (rounded [1, 0]). Should be [1, 0].\n"
     ]
    }
   ],
   "source": [
    "for inp, target in zip(binary_2, ha_target):\n",
    "    prediction = ha_network.feed_forward(inp)\n",
    "    rounded_prediction = list(map(round, prediction))\n",
    "    print(f\"Input: {inp}. Network made prediction {prediction} (rounded {rounded_prediction}). Should be {target}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the network was able to figure it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Classifying the IRIS-dataset:\n",
    "\n",
    "Like in assignment 2, we'll be attempting to classify the Iris-dataset. This time however, we have the power of a fully connected network that's able to learn: therefore, we can attempt to classify all classes, instead of between 2 at a time. We'll import SKLearn, but just to import (and gain some insight into) the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True).frame\n",
    "\n",
    "iris.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[\"target\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable has 3 permutations, which we'll be attempting to classify. This means we'll have 3 neurons in the output layer. We'll one-hot encode these so that we can actually use our network to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iris_target = [[0, 0, 0] for record in iris[\"target\"]]\n",
    "\n",
    "for new_target, old_target in zip(iris_target, iris[\"target\"]):\n",
    "    new_target[old_target] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal length (cm)    float64\n",
       "sepal width (cm)     float64\n",
       "petal length (cm)    float64\n",
       "petal width (cm)     float64\n",
       "target                 int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_features_df = iris.drop(columns=\"target\")\n",
    "iris_features_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 numerical inputs, which are already the proper datatype, which we can directly use. For good measure, we'll turn these into Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iris_features = []\n",
    "for _, record in iris_features_df.iterrows():\n",
    "    iris_features.append(tuple(val for val in record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test split\n",
    "\n",
    "To make sure we don't over fit our network, we can split the dataset into training and testing sets. I'm still not quite sure whether we're allowed to use external modules for things like this, therefore, we'll be using the random module manually for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we combine the features and target, and shuffle that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iris_combined_list = list(zip(iris_features, iris_target))\n",
    "random.shuffle(iris_combined_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the features and target again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iris_features, iris_target = zip(*iris_combined_list)\n",
    "iris_features, iris_target = list(iris_features), list(iris_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having randomly shuffled the features/targets, we can split these into a training and testing set. How many datapoints do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150 records. Using 120 for training and 30 for testing seems like a good proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_split_i = 30\n",
    "\n",
    "iris_features_train = iris_features[_split_i:]\n",
    "iris_features_test = iris_features[:_split_i]\n",
    "\n",
    "iris_target_train = iris_target[_split_i:]\n",
    "iris_target_test = iris_target[:_split_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Creating the network\n",
    "\n",
    "Like mentioned before, we have 4 inputs, and 3 outputs. Considering the Perceptrons in P2 were almost able to classify between individual classes, I don't expect we'll need a terribly complicated network. Therefore, we'll be using the following architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iris_network = ml.NeuronNetwork.build_network(\n",
    "    [4, 8, 8, 3],\n",
    "    0.01,\n",
    "    ml.sigmoid,\n",
    "    xavier_initialization=True\n",
    ")\n",
    "\n",
    "iris_network.train(\n",
    "    iris_features_train,\n",
    "    iris_target_train,\n",
    "    iterations=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iris_outputs = [iris_network.feed_forward(inp) for inp in iris_features_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ofcourse, our Network still outputs floats. We'll round the outputs to actually interpret the predictions, and view the accuracy over the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_outputs = list(map(ml.round_iterable, iris_outputs))\n",
    "\n",
    "ml.accuracy(iris_outputs, iris_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 100% accuracy, which is not entirely unusual.\n",
    "\n",
    "### Classifying the Digit dataset.\n",
    "\n",
    "The Digit dataset, similar to the MNIST dataset, has handwritten digits. The Digit dataset has 8x8 images however. We can import it through SKLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
       "0        0.0        0.0        5.0       13.0        9.0        1.0   \n",
       "1        0.0        0.0        0.0       12.0       13.0        5.0   \n",
       "2        0.0        0.0        0.0        4.0       15.0       12.0   \n",
       "3        0.0        0.0        7.0       15.0       13.0        1.0   \n",
       "4        0.0        0.0        0.0        1.0       11.0        0.0   \n",
       "\n",
       "   pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_7  pixel_7_0  \\\n",
       "0        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "2        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "3        0.0        0.0        0.0        8.0  ...        0.0        0.0   \n",
       "4        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "\n",
       "   pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  pixel_7_6  \\\n",
       "0        0.0        6.0       13.0       10.0        0.0        0.0   \n",
       "1        0.0        0.0       11.0       16.0       10.0        0.0   \n",
       "2        0.0        0.0        3.0       11.0       16.0        9.0   \n",
       "3        0.0        7.0       13.0       13.0        9.0        0.0   \n",
       "4        0.0        0.0        2.0       16.0        4.0        0.0   \n",
       "\n",
       "   pixel_7_7  target  \n",
       "0        0.0       0  \n",
       "1        0.0       1  \n",
       "2        0.0       2  \n",
       "3        0.0       3  \n",
       "4        0.0       4  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits(as_frame=True).frame\n",
    "digits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_unique = digits[\"target\"].unique()\n",
    "amount_of_digits = digits_unique.shape[0]\n",
    "digits_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 different digits to be classified, which means our network will have an output layer consisting of 10 neurons. Like with the iris dataset, we'll one-hot encode these, so that the network treats these as being ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "digit_target = [[0 for _ in range(amount_of_digits)] for _ in digits[\"target\"]]\n",
    "\n",
    "for new_target, old_target in zip(digit_target, digits[\"target\"]):\n",
    "    new_target[old_target] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pixel_0_0    float64\n",
       "pixel_0_1    float64\n",
       "pixel_0_2    float64\n",
       "pixel_0_3    float64\n",
       "pixel_0_4    float64\n",
       "              ...   \n",
       "pixel_7_4    float64\n",
       "pixel_7_5    float64\n",
       "pixel_7_6    float64\n",
       "pixel_7_7    float64\n",
       "target         int32\n",
       "Length: 65, dtype: object"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datatypes are already correct again. Let's convert these features to Python native types, because that is what the ml package was developed for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "digit_features_df = digits.drop(columns=\"target\")\n",
    "digit_features = []\n",
    "for _, record in digit_features_df.iterrows():\n",
    "    digit_features.append(tuple(val for val in record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test split\n",
    "\n",
    "Even though the assigment doesn't specify it, I will split the dataset into seperate training and testing datasets, because there's just nothing interesting about an overfitted network. We'll go through the same procedure as with the iris dataset, so I won't need to ellaborate in the individual steps much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "digit_combined_list = list(zip(digit_features, digit_target))\n",
    "random.shuffle(digit_combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "digit_features, digit_target = zip(*digit_combined_list)\n",
    "digit_features, digit_target = list(digit_features), list(digit_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 65)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a bit more then 20% as test set, so we'll take 400 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_split_i = 400\n",
    "\n",
    "digit_features_train = digit_features[_split_i:]\n",
    "digit_features_test = digit_features[:_split_i]\n",
    "\n",
    "digit_target_train = digit_target[_split_i:]\n",
    "digit_target_test = digit_target[:_split_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the network\n",
    "\n",
    "Classifying digits is a more complicated task then classifying the earlier iris dataset. Therefore, I'll be using a more extensive network. The network has 8 x 8 = 64 inputs, a hidden layer with 64 neurons, a hidden layer with 20 neurons, and an output layer with 10 neurons, representing all one-hot encoded targets. Once again, we'll be using xavier initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "digit_network = ml.NeuronNetwork.build_network(\n",
    "    [64, 64, 20, 10],\n",
    "    0.1,\n",
    "    ml.sigmoid,\n",
    "    xavier_initialization=True\n",
    ")\n",
    "\n",
    "digit_network.train(\n",
    "    digit_features_train,\n",
    "    digit_target_train,\n",
    "    iterations=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network took approximately. 15 minutes. Once again, we'll be using accuracy to review the network's performance on the test-dataset. Let's feed it to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "digit_outputs = [digit_network.feed_forward(inp) for inp in digit_features_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll round the outputs once again, and fetch the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_outputs = list(map(ml.round_iterable, digit_outputs))\n",
    "\n",
    "ml.accuracy(digit_outputs, digit_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "96.5%, that's wonderful! For my own curiosity: I wonder what accuracy it would get on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992841803865425"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_train_outputs = [digit_network.feed_forward(inp) for inp in digit_features_train]\n",
    "\n",
    "digit_train_outputs = list(map(ml.round_iterable, digit_train_outputs))\n",
    "\n",
    "ml.accuracy(digit_train_outputs, digit_target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "99%. This could mean that the network remembers (almost) all training examples, although we can't be sure. The network does have significantly less neurons then there are training examples, but who knows.\n",
    "\n",
    "This could be explored by checking the accuracy, or a loss metric, on both the train and test datasets after every n epochs, and seeing whether the test dataset performance stagnates or worsens as the train dataset performance keeps increasing, but that's not quite in scope of this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}