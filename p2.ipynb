{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "For P2, we've been instructed to implement the perceptron learning rule to the Perceptron that was implemented earlier. The python source code contains this functionality now. All that remains is once again confirming it's functionality through some examples/tests.\n",
    "\n",
    "This time, these are the requirements:\n",
    "\n",
    "<ol>\n",
    "<li>Having a perceptron teach itself to be an AND-gate.</li>\n",
    "<li>Having a perceptron teach itself to be an XOR-gate.</li>\n",
    "<li>Having a perceptron teach itself to classify the <i>Iris-dataset</i></li>\n",
    "</ol>\n",
    "\n",
    "From now on, I won't be using an \"automated test framework\", as per the valid criticism of the Teacher's assistant, because that arguably needed to be tested on it's own to be valid.\n",
    "\n",
    "I will borrow a modification of last assignment's \"binary_input_space\" function. I'll also import some packages, and the python source code, then I'll work through the required tests one by one.\n",
    "\n",
    "We'll also set the random seed used for initializing Perceptron with random weights/bias values, so that the results of running this notebook are reproducible. Even though it's only required for the third assignment, I'll set the random seed at my student number."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "random.seed(1792206)\n",
    "\n",
    "import ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "outputs": [],
   "source": [
    "def binary_input_space(length: int) -> List[Tuple[int]]:\n",
    "    \"\"\"Compute all possible binary input combinations with a certain length.\n",
    "\n",
    "    Args:\n",
    "        length: length of the combinations.\n",
    "\n",
    "    Returns:\n",
    "        All possible binary input combinations of a certain length.\"\"\"\n",
    "    return list(itertools.product([0, 1], repeat=length))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>AND-gate</h2>\n",
    "\n",
    "First, we'll initialize a perceptron with random values for it's (2) weights and bias, and a learning rate of 0.1 (assignment specifications)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "outputs": [],
   "source": [
    "p_and = ml.Perceptron.random_instance(weights_amount=2, learning_rate=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll construct the binary input space, and add the expected output for an AND-gate to it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0, 0), (0, 1), (1, 0), (1, 1)]"
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "and_input_space = binary_input_space(2)\n",
    "and_input_space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 0, 0, 1]"
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "and_targets = [a and b for a, b in and_input_space]\n",
    "and_targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we'll have the perceptron's learn using the entire dataset, until it's converged. We use the mean squared error as loss-metric, which is a floating-point number. Because floating-point maths can be unreliable, we'll set the threshold after which we assume convergence at a very small, but non-0 number. Because the training dataset consists of only 4 records, the mse cannot reach a value that would be below this value, without the perceptron's learning having converged."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_and.learn_until_loss(inputs=and_input_space,\n",
    "                       targets=and_targets,\n",
    "                       loss_target=0.00000000001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We indeed reach a loss of 0. This leaves the parameters of the perceptron at:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "outputs": [
    {
     "data": {
      "text/plain": "Perceptron: b: -0.7076041591715243. w: [0.5727410173077363, 0.19976015294456012] )"
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_and"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>XOR-gate</h2>\n",
    "\n",
    "Next, we'll (attempt) to have a perceptron teach itself to function as a XOR-gate. Considering the fact that perceptrons can only correctly solve problems that consist of a (binary) target variable that can be seperated with 1 linear seperation, we know that an entirely correctly configured perceptron could never actually mimic the functionality of a XOR-gate.\n",
    "\n",
    "We start by initializing another perceptron."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "outputs": [],
   "source": [
    "p_xor = ml.Perceptron.random_instance(weights_amount=2, learning_rate=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And defining the input space and targets of our training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0, 0), (0, 1), (1, 0), (1, 1)]"
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xor_input_space = binary_input_space(2)\n",
    "xor_input_space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1, 1, 0]"
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xor_targets = [a ^ b for a, b in xor_input_space]\n",
    "xor_targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Like mentioned before, learning until results are pointless is futile. Although we won't limit ourselves by learning until a certain imperfect loss is reached. We'll learn by iterating through the dataset 1000 times, after which if convergence hasn't been reached, we probably never will."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5"
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_xor.learn_iterations(inputs=xor_input_space,\n",
    "                       targets=xor_targets,\n",
    "                       iterations=1000)\n",
    "\n",
    "p_xor.loss(xor_input_space, xor_targets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Like expected, the perceptron isn't doing a terribly good job of being a XOR-gate.\n",
    "\n",
    "We would expect a perceptron to be able to do better. With a single linear separation, creating (something like) an OR-gate should be possible, with would leave us with only a 0.25 MSE, because only 1 output would be wrong. Even though with the right parameters and some luck it might be possible to reach this point, there is no reason to expect this.\n",
    "\n",
    "This leaves us with the following parameters:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "outputs": [
    {
     "data": {
      "text/plain": "Perceptron: b: 0.03999106408891609. w: [-0.1014128753624709, -0.03368069936568971] )"
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_xor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>IRIS-dataset</h2>\n",
    "\n",
    "The iris dataset is a simple dataset, best described by looking at it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "outputs": [
    {
     "data": {
      "text/plain": "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                5.1               3.5                1.4               0.2   \n1                4.9               3.0                1.4               0.2   \n2                4.7               3.2                1.3               0.2   \n3                4.6               3.1                1.5               0.2   \n4                5.0               3.6                1.4               0.2   \n\n   target  \n0       0  \n1       0  \n2       0  \n3       0  \n4       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True).frame\n",
    "\n",
    "iris.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We've been asked to do 2 things: train a perceptron to distinguish between target variables 0 and 2, and to train a perceptron to distinguish between target variables 1 and 2. We'll create 2 seperate datasets for these assigments. Then we'll split these into their respective inputs and outputs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "outputs": [],
   "source": [
    "iris_i = iris[(iris[\"target\"] == 0) | (iris[\"target\"] == 2)]\n",
    "iris_ii = iris[(iris[\"target\"] == 1) | (iris[\"target\"] == 2)]\n",
    "\n",
    "iris_i_target = iris_i[\"target\"].values.tolist()\n",
    "iris_i_input = iris_i.drop(columns=\"target\").values.tolist()\n",
    "\n",
    "iris_ii_target = iris_ii[\"target\"].values.tolist()\n",
    "iris_ii_input = iris_ii.drop(columns=\"target\").values.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Iris I: Distinguising between 0 and 2</h4>\n",
    "\n",
    "The target variable is either 0 or 2. Our perceptron is implemented to work with 0 or 1. Therefore, we need to replace all the 2s with 1s."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "outputs": [],
   "source": [
    "iris_i_target = [1 if num == 2 else 0 for num in iris_i_target]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because we're not sure whether convergence will be reached, we'll initialize a perceptron and train it for 1000 iterations using the entire dataset. Because perceptron"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_iris_i = ml.Perceptron.random_instance(weights_amount=4, learning_rate=0.1)\n",
    "\n",
    "p_iris_i.learn_iterations(inputs=iris_i_input,\n",
    "                          targets=iris_i_target,\n",
    "                          iterations=1000)\n",
    "\n",
    "p_iris_i.loss(inputs=iris_i_input,\n",
    "              targets=iris_i_target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It appears as though our perceptron is able to reach convergence, which means the classes are linearly separable. This leaves us with the following perceptron:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "outputs": [
    {
     "data": {
      "text/plain": "Perceptron: b: -0.7617766038964625. w: [0.06113663314870588, -0.9477548793919288, 0.6743390138402527, 1.0258695150659176] )"
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_iris_i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h4>Iris II: Distinguishing between 1 and 2</h4>\n",
    "\n",
    "We'll follow the same procedure as we did before, first renaming our target variable to 0s and 1s, and then training for a certain amount of iterations.\n",
    "\n",
    "Our target variable consists of 1s and 2s. Therefore, we can subtract one from everything:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "outputs": [],
   "source": [
    "iris_ii_target = [num - 1 for num in iris_ii_target]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "outputs": [
    {
     "data": {
      "text/plain": "0.04"
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_iris_ii = ml.Perceptron.random_instance(weights_amount=4, learning_rate=0.1)\n",
    "\n",
    "p_iris_ii.learn_iterations(inputs=iris_ii_input,\n",
    "                           targets=iris_ii_target,\n",
    "                           iterations=1000)\n",
    "\n",
    "p_iris_ii.loss(inputs=iris_ii_input,\n",
    "              targets=iris_ii_target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are still left with a mean squared error of 0.04, which means our perceptron wasn't able to converge. This means the classes are not linearly separable, and a single perceptron can't predict them perfectly."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}